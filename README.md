# ğŸ§  Instant-NGP (TinyNeRF Implementation)

A lightweight, educational reimplementation of **NVIDIAâ€™s Instant Neural Graphics Primitives (Instant-NGP)** â€” focusing on **Neural Radiance Fields (NeRF)**.  
This project demonstrates how a neural network can **learn a 3D scene from 2D images** and **render novel views** from unseen camera angles.

---

## ğŸš€ Overview

This project builds a **TinyNeRF** model â€” a simple MLP network â€” that takes encoded 3D positions and view directions as input and predicts:
- **Density (Ïƒ)** â†’ how much light is absorbed/scattered
- **Color (RGB)** â†’ emitted color at that 3D point

By training on multiple images of a scene (like a checkerboard or object from various angles), it learns a full **3D radiance field**, allowing **photorealistic novel-view rendering**.

---

## ğŸ§© Features

âœ… Modular structure with PyTorch  
âœ… Lightweight **TinyNeRF** MLP backbone  
âœ… Simple **training & rendering** interface  
âœ… Extendable to **HashGrid Encoding** and **Occupancy Grid Sampling**  
âœ… Inspired by **NVIDIA Instant-NGP** & **Google TinyNeRF**  

---

## ğŸ—‚ï¸ Project Structure

instant-ngp-project/
â”‚
â”œâ”€â”€ main.py # Entry point (train or render)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ hash_encoder.py # Positional or hash encoding
â”‚ â”œâ”€â”€ tiny_nerf.py # Core NeRF model (MLP)
â”‚ â”œâ”€â”€ train.py # Training loop and loss
â”‚ â””â”€â”€ utils.py # Helper utilities
â”‚
â”œâ”€â”€ data/ # Input images and camera poses
â”œâ”€â”€ outputs/ # Trained models and rendered images
â”œâ”€â”€ venv/ # Virtual environment
â””â”€â”€ README.md # Project documentation

---

## âš™ï¸ Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/yourusername/instant-ngp-project.git
   cd instant-ngp-project
    cd instant-ngp-project
2. **Create virtual environment**
   ```bash
    python -m venv npgenv
    npgenv\Scripts\activate
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
## How it works
1ï¸âƒ£ Data Encoding

3D coordinates and viewing directions are first encoded using a positional or hash encoder to capture fine spatial details.

2ï¸âƒ£ TinyNeRF Model

A small MLP takes these encoded features and predicts:

(color_rgb, density_sigma) = TinyNeRF(encoded_xyz_dir)

3ï¸âƒ£ Volume Rendering

For each camera ray (through a pixel), the network samples multiple points in 3D space and integrates the predicted colors and densities along that ray to compute the final pixel color.

4ï¸âƒ£ Training

The model minimizes Mean Squared Error (MSE) between the rendered pixels and the ground truth image pixels.

5ï¸âƒ£ Rendering

After training, the model can render the same scene from new camera viewpoints â€” effectively performing 3D reconstruction from 2D inputs.

## Usage
**ğŸ‹ï¸ Train the model**
```bash
python main.py
```
When prompted:

Enter mode (train/render): train

**ğŸ¨ Render a scene**

After training:
```bash
python main.py
```

Then choose:

Enter mode (train/render): render


A rendered image will be saved as:

outputs/render.png

**ğŸ–¼ï¸ Example Results**
Input Views	Reconstructed Scene

The network reconstructs the 3D geometry and appearance from multiple 2D images.

**ğŸ”§ Future Improvements**

 Implement multi-resolution hash encoding (Instant-NGP style)

 Add occupancy grid sampling for faster convergence

 Support real NeRF datasets (e.g., LLFF, Blender scenes)

 Integrate interactive GUI rendering with Open3D or pythreejs

 Add camera pose optimization for uncalibrated images

**Author**

Developer: Zahra Alipour
ğŸ“§ Email: zahraalipour.ac@gmail.com

Inspired by:

NVIDIA Instant-NGP (2022)

TinyNeRF (Google Research)
